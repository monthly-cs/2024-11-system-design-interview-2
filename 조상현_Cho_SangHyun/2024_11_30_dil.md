191p ~ 206p

처음엔 광고 클릭 이벤트 집계 시스템이란 게 낯설었는데, 읽어보니 **대량의 이벤트가 발생할 때 그 로그를 수집하고 집계하는 시스템의 설계 방법**으로 이해했습니다.

### 1. 입력된 로그 데이터의 저장 & S3 스토리지 클래스
백업을 위해 원시 로그 데이터를 저장하는 것을 말하며, 이 놈이 저장되는 저장소는 쓰기 요청 처리 위주이므로 책에선 쓰기에 최적화된 카산드라 혹은 시계열 DB인 인플럭스가 적절함을 설명합니다.  또한 ORC, Parquet같은 형식으로 저장 시 AWS S3에도 저장 가능하다고 설명합니다. (이유 : Athena 등의 툴로 S3에 저장된 데이터를 분석할 수 있는데, 이를 위해선 Parquet이나 ORC같은 칼럼 기반 포맷으로 저장하는 게 유리.) 

그리고 원시 데이터를 콜드 스토리지로 옮기면 비용 절감이 가능함을 언급합니다. 전에 S3의 스토리지 클래스를 공부한 기억이 있어 복습 겸 다시 살펴봤습니다.

- S3 Standard (Default)
    - 자주 액세스하는 데이터의 일반적인 스토리지 등급
- S3 Standard-IA(Infrequent Access)
    - 장기간 사용하나 액세스 빈도는 낮은 데이터용 등급
- S3 One Zone-IA
    - 장기간 사용하나 액세스 빈도는 낮은 데이터용. 근데 가용 영역 하나에만 저장
- S3 Glacier Instant Retrieval
    - 거의 액세스하진 않는데, 필요할 때 드는 복원 시간은 좀 빨라야 할 때(밀리초 단위)
- S3 Glacier Flexible Retrieval
    - 거의 액세스하지 않고, 복원 시간도 몇 분 ~ 몇 시간이면 충분할 때
- S3 Glacier Deep Archive
    - 거의 액세스 안 함. 12시간 이내에 복원됨. 스토리지 아카이브나 디지털 콘텐츠 보관 용도

S3 수명 주기 정책을 활용하면 처음엔 Standard에 뒀다가 1달 뒤 Glacier로 옮기는 것 등등이 가능합니다(특정 기간 동안만 자주 사용하는 문서가 있다든지 할 때 활용 가능). 책에서 다루는 시스템에선 집계 데이터가 손실되는 경우 등에만 원시 데이터에 자주 접근하는데.. 감히 추측컨대 그 빈도가 낮을 거 같습니다. 단순하게만 보면 첨에 Standard-IA에 저장했다가 일정 시간이 지나면 Glacier Flexible Retrieval 등으로 옮기는 정책을 사용해보면 좋을 것 같습니다. 옮기는 기간 설정이 애매하면 `Intelligent Tiering`을 사용해도 될 것 같습니다.

<br/>  


### 2. 맵리듀스 & 스타스키마
맵리듀스는 구글에서 대용량 데이터 처리를 분산 병렬 컴퓨팅으로 처리하려고 만든 프레임워크를 말한다고 합니다. 책에 나오는 것처럼 map이 분산 처리, reduce가 다시 합치는 것으로 일종의 분할정복 개념으로 이해했습니다.

스타스키마는 처음 듣는 용어라 생소해서 조사해봤습니다. DW에서 데이터 분석의 편의를 위해 Fact Table(분석 대상이 되는 테이블)을 중앙에 두고 Dimension Table(해당 테이블의 데이터를 설명하는 테이블)을 주변에 두는 형태인데, 별 모양 같다고 해서 스타 스키마라고 부른다고 합니다. Dimension이라 부르는 이유는 **데이터를 분석할 때 데이터를 바라보는 관점or목적에 따라 다른 차원에서 본다고 여겨서** 그렇다고 합니다. (개인적으로 회사 DW 쪽에도 디멘젼 테이블로 이름붙은 애들이 엄청 많은데.. 디멘젼이 어떤 의미인지를 알게 되어 흥미로웠습니다 :) )

![image](https://github.com/user-attachments/assets/38e27933-fac8-4d64-b45d-a6d7cf9c839f)


책에서의 예로 들면 ad_id, click_minute, count만 있는 게 Fact Table이고, 국가별 분석이라는 관점에서 볼 때 country가 추가된 테이블을 둘 수 있고 이게 국가 Dimension Table이 되는 것으로 이해했습니다.




