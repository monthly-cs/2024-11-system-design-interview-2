~~54p

⛳️ 세줄 평!

1. 
2. 
3. 

# 3단계 상세 설계

## 3.1. 접근 전략

가상 설계 면접을 `30분`으로 진행한다면 남은 시간은 `13~15분`입니다.

이 시간 동안 `핵심 아키텍처 1건`과 `보조 아키텍처 1건`에 대한 딥다이브 논의가 가능합니다.

저라면 `위치 갱신을 위한 Redis pub/sub`과 `위치 로그를 위한 OLAP DB`를 선택하겠습니다.

## 3.2. 실제 접근

상세 설계에는 실제 제품군을 기준으로 설명을 이어가겠습니다. <br>
따라서 `Redis Pub/Sub Channel`과 위치 로그에 사용할 `Kinesis Datastream, S3, RedShift Spectrum`에 대해 다룹니다.

### 3.2.1. 위치 갱신을 위한 Redis Pub/Sub

1. Redis Pub/Sub 패턴에서 사용될 데이터 구조 설계
2. Redis Pub/Sub 패턴에서 필요한 데이터 용량 추정
3. Redis Pub/Sub 패턴에서 필요한 CPU 용량 추정
4. Redis Pub/Sub 패턴에서 채널 배정의 문제
5. Redis Pub/Sub 패턴에서 채널(==친구)의 증감으로 인한 배정 문제
6. Redis Pub/Sub 패턴에서 클러스터 증감으로 인한 배정 문제

### 3.2.2. 위치 로그 기록을 위한 OLAP DB

1. 위치 로그의 수집 용량 추정
3. 위치 로그의 수집 아키텍처 설계
4. 위치 로그의 조회 아키텍처 설계

#### 3.2.2.1. 위치 로그의 초당 수집 용량 추정

이미 [2.3.3. 기술적인 난제를 해결하기 위한 핵심 아키텍처 설계 - 조회를 처리하는 방안](./2024_11_12_dil.md#223-기술적인-난제를-해결하기-위한-핵심-아키텍처-설계---조회를-처리하는-방안)에서 <br>
전체 유저가 30초 동안 생성하는 위치 정보의 크기가 5 GB 임을 알았습니다. <br>
시간당 `600GB`, 일당 `14.40 TB`, 월별 `439.20 TB`, 연당 `5.25 PB`가 저장됩니다.

> 5 GB/30 s * 3600s/h = 600 GB/h
> 600 GB/h * 24h/d = 14,400 GB/d / 1000 TB/GB = `14.40 TB/d`
> 14,400 GB * 30.5 d/m = 439,200 GB/m / 1000 TB/GB = `439.20 TB/m`
> 14,400 GB * 365.25 d/y = 5,259,600 GB/m / 1000 TB/GB / 1000 PB/TB = `5.25 PB`

이 데이터를 형변환, 압축 없이 일반적인 OLTP에 저장하는 것은 너무 큰 오버헤드입니다. <br>
데이터 유형(로그)과 수집 목적(머신러닝)을 고려하면 OLAP DB를 사용하는 방법이 있습니다.

#### 3.2.2.2. 위치 로그의 수집 아키텍처 설계

높은 네트워크 대역폭을 감당하기 위해서는 `클러스터 스트림`과 같은 기능이 지원되어야 합니다. <br>
Amazon Kinesis Datastream을 사용하면 트래픽 요청량에 맞춰서 다수의 샤드를 생성할 수 있습니다.<br>

이후 해당 데이터는 Amazon Kinesis Firehose를 경유(옵션)하여 S3에 저장할 수 있습니다. <br>
또한 이 과정에서 gzip 압축, parquet 형변환 등으로 최종 10배 정도 저장 용량을 감소시킬 수 있습니다.

#### 3.2.2.3. 위치 로그의 조회 아키텍처 설계

S3에 저장된 로그 데이터를 조회하기 위해서는 크게 2가지 방법이 존재합니다.

1. Amazon Athena Query
2. RedShift Spectrum (Cluster or Serverless)

대규모 머신러닝을 돌리는 상황에서는 작동 시작 시점부터 대량의 데이터를 조회하게 될 것입니다. <br>
이 경우 사용량당 비용이 나오는 Amazon Athena Query는 적합하지 않습니다. <br>
또한 RedShift Serverless도 동일 시간 사용량 대비 11배 이상 비싼 케이스가 존재할 수 있어 부적합합니다.

최종적으로 RedShift Cluster를 사용하는 것이 합리적으로 보입니다.